{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simon\\iCloudDrive\\Documents\\Github\\PyTorch_Deep_Learning_ITE-5410\\env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "NUM_INPUT = 2\n",
    "NUM_HIDDEN = 5\n",
    "NUM_OUT = 1\n",
    "LR = 0.01\n",
    "EPOCH = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1., 1.], [0., 0.], [1., 0.], [0., 1.]])\n",
    "y = torch.tensor([[0.], [0.], [1.], [1.]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    # define input layer to a hidden layer\n",
    "    nn.Linear(NUM_INPUT, NUM_HIDDEN),\n",
    "    nn.ReLU(),      # activation func\n",
    "    # define hidden layer to output layer\n",
    "    nn.Linear(NUM_HIDDEN, NUM_OUT)\n",
    "    # can apply activation func here. if not, defuat af will apply.\n",
    ")\n",
    "\n",
    "loss_fun = torch.nn.MSELoss()   # use MSE as the loss function\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)     # algorithm\n",
    "# pass the parameter to the optimizer\n",
    "# since we have 1 hidden layer, parameters are w1, w2, b1, b2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.9343337416648865\n",
      "epoch: 1, loss: 0.8891775012016296\n",
      "epoch: 2, loss: 0.8467819094657898\n",
      "epoch: 3, loss: 0.806969165802002\n",
      "epoch: 4, loss: 0.7699593305587769\n",
      "epoch: 5, loss: 0.73472660779953\n",
      "epoch: 6, loss: 0.7010949850082397\n",
      "epoch: 7, loss: 0.6689873933792114\n",
      "epoch: 8, loss: 0.6383196115493774\n",
      "epoch: 9, loss: 0.6090022325515747\n",
      "epoch: 10, loss: 0.5809432864189148\n",
      "epoch: 11, loss: 0.5540527105331421\n",
      "epoch: 12, loss: 0.5282474756240845\n",
      "epoch: 13, loss: 0.503457248210907\n",
      "epoch: 14, loss: 0.479628324508667\n",
      "epoch: 15, loss: 0.45672744512557983\n",
      "epoch: 16, loss: 0.43474239110946655\n",
      "epoch: 17, loss: 0.41368141770362854\n",
      "epoch: 18, loss: 0.39357078075408936\n",
      "epoch: 19, loss: 0.3744509518146515\n",
      "epoch: 20, loss: 0.35637253522872925\n",
      "epoch: 21, loss: 0.33939170837402344\n",
      "epoch: 22, loss: 0.32356590032577515\n",
      "epoch: 23, loss: 0.3089498281478882\n",
      "epoch: 24, loss: 0.29559147357940674\n",
      "epoch: 25, loss: 0.2835281491279602\n",
      "epoch: 26, loss: 0.2727821469306946\n",
      "epoch: 27, loss: 0.2639274299144745\n",
      "epoch: 28, loss: 0.2566092312335968\n",
      "epoch: 29, loss: 0.2507331371307373\n",
      "epoch: 30, loss: 0.24620839953422546\n",
      "epoch: 31, loss: 0.24290645122528076\n",
      "epoch: 32, loss: 0.24066314101219177\n",
      "epoch: 33, loss: 0.239284485578537\n",
      "epoch: 34, loss: 0.23855571448802948\n",
      "epoch: 35, loss: 0.23825347423553467\n",
      "epoch: 36, loss: 0.23815912008285522\n",
      "epoch: 37, loss: 0.23807236552238464\n",
      "epoch: 38, loss: 0.23782309889793396\n",
      "epoch: 39, loss: 0.23728026449680328\n",
      "epoch: 40, loss: 0.23748067021369934\n",
      "epoch: 41, loss: 0.23774486780166626\n",
      "epoch: 42, loss: 0.237665057182312\n",
      "epoch: 43, loss: 0.23723925650119781\n",
      "epoch: 44, loss: 0.2364898920059204\n",
      "epoch: 45, loss: 0.235458105802536\n",
      "epoch: 46, loss: 0.2341969907283783\n",
      "epoch: 47, loss: 0.23276552557945251\n",
      "epoch: 48, loss: 0.2312227338552475\n",
      "epoch: 49, loss: 0.22962328791618347\n",
      "epoch: 50, loss: 0.22801409661769867\n",
      "epoch: 51, loss: 0.22643247246742249\n",
      "epoch: 52, loss: 0.2249050736427307\n",
      "epoch: 53, loss: 0.22344812750816345\n",
      "epoch: 54, loss: 0.22206827998161316\n",
      "epoch: 55, loss: 0.22076402604579926\n",
      "epoch: 56, loss: 0.21952733397483826\n",
      "epoch: 57, loss: 0.2183455526828766\n",
      "epoch: 58, loss: 0.2172197699546814\n",
      "epoch: 59, loss: 0.21628788113594055\n",
      "epoch: 60, loss: 0.21527725458145142\n",
      "epoch: 61, loss: 0.21417833864688873\n",
      "epoch: 62, loss: 0.2129843682050705\n",
      "epoch: 63, loss: 0.21169137954711914\n",
      "epoch: 64, loss: 0.21042928099632263\n",
      "epoch: 65, loss: 0.2091997265815735\n",
      "epoch: 66, loss: 0.2079259753227234\n",
      "epoch: 67, loss: 0.20660945773124695\n",
      "epoch: 68, loss: 0.20525255799293518\n",
      "epoch: 69, loss: 0.20385855436325073\n",
      "epoch: 70, loss: 0.20243090391159058\n",
      "epoch: 71, loss: 0.20097331702709198\n",
      "epoch: 72, loss: 0.19967591762542725\n",
      "epoch: 73, loss: 0.1986140012741089\n",
      "epoch: 74, loss: 0.19716614484786987\n",
      "epoch: 75, loss: 0.19537347555160522\n",
      "epoch: 76, loss: 0.19414997100830078\n",
      "epoch: 77, loss: 0.1931324601173401\n",
      "epoch: 78, loss: 0.1919710636138916\n",
      "epoch: 79, loss: 0.19068101048469543\n",
      "epoch: 80, loss: 0.18927696347236633\n",
      "epoch: 81, loss: 0.18777288496494293\n",
      "epoch: 82, loss: 0.18618184328079224\n",
      "epoch: 83, loss: 0.1849226951599121\n",
      "epoch: 84, loss: 0.18371346592903137\n",
      "epoch: 85, loss: 0.1822586953639984\n",
      "epoch: 86, loss: 0.18057969212532043\n",
      "epoch: 87, loss: 0.1791580617427826\n",
      "epoch: 88, loss: 0.1778629720211029\n",
      "epoch: 89, loss: 0.17658010125160217\n",
      "epoch: 90, loss: 0.17524512112140656\n",
      "epoch: 91, loss: 0.17388446629047394\n",
      "epoch: 92, loss: 0.1724531650543213\n",
      "epoch: 93, loss: 0.17095434665679932\n",
      "epoch: 94, loss: 0.16957765817642212\n",
      "epoch: 95, loss: 0.16811543703079224\n",
      "epoch: 96, loss: 0.1665744185447693\n",
      "epoch: 97, loss: 0.16503135859966278\n",
      "epoch: 98, loss: 0.16368457674980164\n",
      "epoch: 99, loss: 0.16217723488807678\n",
      "epoch: 100, loss: 0.16075359284877777\n",
      "epoch: 101, loss: 0.15938347578048706\n",
      "epoch: 102, loss: 0.15794506669044495\n",
      "epoch: 103, loss: 0.15650925040245056\n",
      "epoch: 104, loss: 0.15510408580303192\n",
      "epoch: 105, loss: 0.15381357073783875\n",
      "epoch: 106, loss: 0.15242759883403778\n",
      "epoch: 107, loss: 0.15095651149749756\n",
      "epoch: 108, loss: 0.14941145479679108\n",
      "epoch: 109, loss: 0.1479502022266388\n",
      "epoch: 110, loss: 0.1464693248271942\n",
      "epoch: 111, loss: 0.14500792324543\n",
      "epoch: 112, loss: 0.1435297727584839\n",
      "epoch: 113, loss: 0.14189785718917847\n",
      "epoch: 114, loss: 0.14034384489059448\n",
      "epoch: 115, loss: 0.13876622915267944\n",
      "epoch: 116, loss: 0.13708162307739258\n",
      "epoch: 117, loss: 0.135245680809021\n",
      "epoch: 118, loss: 0.13358600437641144\n",
      "epoch: 119, loss: 0.13184690475463867\n",
      "epoch: 120, loss: 0.130276620388031\n",
      "epoch: 121, loss: 0.1284884512424469\n",
      "epoch: 122, loss: 0.12678351998329163\n",
      "epoch: 123, loss: 0.12501098215579987\n",
      "epoch: 124, loss: 0.1234014481306076\n",
      "epoch: 125, loss: 0.1214926615357399\n",
      "epoch: 126, loss: 0.11969782412052155\n",
      "epoch: 127, loss: 0.11787955462932587\n",
      "epoch: 128, loss: 0.11601696908473969\n",
      "epoch: 129, loss: 0.11423346400260925\n",
      "epoch: 130, loss: 0.11241301894187927\n",
      "epoch: 131, loss: 0.11051984131336212\n",
      "epoch: 132, loss: 0.10860362648963928\n",
      "epoch: 133, loss: 0.10664224624633789\n",
      "epoch: 134, loss: 0.10474692285060883\n",
      "epoch: 135, loss: 0.10305630415678024\n",
      "epoch: 136, loss: 0.10089166462421417\n",
      "epoch: 137, loss: 0.09897886216640472\n",
      "epoch: 138, loss: 0.09701922535896301\n",
      "epoch: 139, loss: 0.09506674110889435\n",
      "epoch: 140, loss: 0.09301862120628357\n",
      "epoch: 141, loss: 0.09099704027175903\n",
      "epoch: 142, loss: 0.0889236330986023\n",
      "epoch: 143, loss: 0.08677477389574051\n",
      "epoch: 144, loss: 0.08462923020124435\n",
      "epoch: 145, loss: 0.08261971175670624\n",
      "epoch: 146, loss: 0.08040769398212433\n",
      "epoch: 147, loss: 0.07846543192863464\n",
      "epoch: 148, loss: 0.07650047540664673\n",
      "epoch: 149, loss: 0.07446032017469406\n",
      "epoch: 150, loss: 0.07235534489154816\n",
      "epoch: 151, loss: 0.07026214897632599\n",
      "epoch: 152, loss: 0.06818422675132751\n",
      "epoch: 153, loss: 0.06600969284772873\n",
      "epoch: 154, loss: 0.06392324715852737\n",
      "epoch: 155, loss: 0.0618131197988987\n",
      "epoch: 156, loss: 0.05966589227318764\n",
      "epoch: 157, loss: 0.05750037729740143\n",
      "epoch: 158, loss: 0.05539410561323166\n",
      "epoch: 159, loss: 0.053376223891973495\n",
      "epoch: 160, loss: 0.05136748403310776\n",
      "epoch: 161, loss: 0.04927081614732742\n",
      "epoch: 162, loss: 0.04733872786164284\n",
      "epoch: 163, loss: 0.045377496629953384\n",
      "epoch: 164, loss: 0.0435197651386261\n",
      "epoch: 165, loss: 0.04165616258978844\n",
      "epoch: 166, loss: 0.03978675976395607\n",
      "epoch: 167, loss: 0.03791910409927368\n",
      "epoch: 168, loss: 0.03606095165014267\n",
      "epoch: 169, loss: 0.034336481243371964\n",
      "epoch: 170, loss: 0.032579455524683\n",
      "epoch: 171, loss: 0.0307578407227993\n",
      "epoch: 172, loss: 0.029062481597065926\n",
      "epoch: 173, loss: 0.02775527350604534\n",
      "epoch: 174, loss: 0.026086099445819855\n",
      "epoch: 175, loss: 0.024451497942209244\n",
      "epoch: 176, loss: 0.02303311601281166\n",
      "epoch: 177, loss: 0.02163681946694851\n",
      "epoch: 178, loss: 0.02026604488492012\n",
      "epoch: 179, loss: 0.01892496645450592\n",
      "epoch: 180, loss: 0.0176180899143219\n",
      "epoch: 181, loss: 0.016350077465176582\n",
      "epoch: 182, loss: 0.015125440433621407\n",
      "epoch: 183, loss: 0.014047485776245594\n",
      "epoch: 184, loss: 0.012940218672156334\n",
      "epoch: 185, loss: 0.011815613135695457\n",
      "epoch: 186, loss: 0.010892851278185844\n",
      "epoch: 187, loss: 0.010021626949310303\n",
      "epoch: 188, loss: 0.009135942906141281\n",
      "epoch: 189, loss: 0.00825182069092989\n",
      "epoch: 190, loss: 0.0075093647465109825\n",
      "epoch: 191, loss: 0.006815548054873943\n",
      "epoch: 192, loss: 0.006159575656056404\n",
      "epoch: 193, loss: 0.005541687831282616\n",
      "epoch: 194, loss: 0.004972366150468588\n",
      "epoch: 195, loss: 0.004438619129359722\n",
      "epoch: 196, loss: 0.003944516181945801\n",
      "epoch: 197, loss: 0.003493847558274865\n",
      "epoch: 198, loss: 0.003078312845900655\n",
      "epoch: 199, loss: 0.0026974293868988752\n",
      "epoch: 200, loss: 0.0023425205145031214\n",
      "epoch: 201, loss: 0.0020279348827898502\n",
      "epoch: 202, loss: 0.001742841675877571\n",
      "epoch: 203, loss: 0.0014834218891337514\n",
      "epoch: 204, loss: 0.0012629050761461258\n",
      "epoch: 205, loss: 0.001067023491486907\n",
      "epoch: 206, loss: 0.0008941543055698276\n",
      "epoch: 207, loss: 0.0007467007962986827\n",
      "epoch: 208, loss: 0.0006198580376803875\n",
      "epoch: 209, loss: 0.0005116924294270575\n",
      "epoch: 210, loss: 0.0004202303825877607\n",
      "epoch: 211, loss: 0.0003435144608374685\n",
      "epoch: 212, loss: 0.0002796552435029298\n",
      "epoch: 213, loss: 0.00022688964963890612\n",
      "epoch: 214, loss: 0.00018361007096245885\n",
      "epoch: 215, loss: 0.00014837965136393905\n",
      "epoch: 216, loss: 0.0001232596841873601\n",
      "epoch: 217, loss: 0.00010170790483243763\n",
      "epoch: 218, loss: 8.31520083011128e-05\n",
      "epoch: 219, loss: 6.742659024894238e-05\n",
      "epoch: 220, loss: 5.4318163165589795e-05\n",
      "epoch: 221, loss: 4.379806341603398e-05\n",
      "epoch: 222, loss: 3.69082554243505e-05\n",
      "epoch: 223, loss: 2.9551050829468295e-05\n",
      "epoch: 224, loss: 2.2602675016969442e-05\n",
      "epoch: 225, loss: 1.8066206393996254e-05\n",
      "epoch: 226, loss: 1.448449165764032e-05\n",
      "epoch: 227, loss: 1.1753842954931315e-05\n",
      "epoch: 228, loss: 9.766479706740938e-06\n",
      "epoch: 229, loss: 8.411864655499812e-06\n",
      "epoch: 230, loss: 7.579513294331264e-06\n",
      "epoch: 231, loss: 7.161341727623949e-06\n",
      "epoch: 232, loss: 7.055845344439149e-06\n",
      "epoch: 233, loss: 7.170424396463204e-06\n",
      "epoch: 234, loss: 7.423701845254982e-06\n",
      "epoch: 235, loss: 7.748125426587649e-06\n",
      "epoch: 236, loss: 8.090053597697988e-06\n",
      "epoch: 237, loss: 8.409445399593096e-06\n",
      "epoch: 238, loss: 8.68017286848044e-06\n",
      "epoch: 239, loss: 8.887068361218553e-06\n",
      "epoch: 240, loss: 9.025152394315228e-06\n",
      "epoch: 241, loss: 9.096588655665983e-06\n",
      "epoch: 242, loss: 9.108242011279799e-06\n",
      "epoch: 243, loss: 9.069905900105368e-06\n",
      "epoch: 244, loss: 8.992334187496454e-06\n",
      "epoch: 245, loss: 8.885546776582487e-06\n",
      "epoch: 246, loss: 8.75782370712841e-06\n",
      "epoch: 247, loss: 8.615302249381784e-06\n",
      "epoch: 248, loss: 8.462168807454873e-06\n",
      "epoch: 249, loss: 8.299975888803601e-06\n",
      "epoch: 250, loss: 8.128932677209377e-06\n",
      "epoch: 251, loss: 7.948036000016145e-06\n",
      "epoch: 252, loss: 7.756100785627495e-06\n",
      "epoch: 253, loss: 7.551639100711327e-06\n",
      "epoch: 254, loss: 7.334089332289295e-06\n",
      "epoch: 255, loss: 7.103451025614049e-06\n",
      "epoch: 256, loss: 6.861208930786233e-06\n",
      "epoch: 257, loss: 6.6090979089494795e-06\n",
      "epoch: 258, loss: 6.3504503486910835e-06\n",
      "epoch: 259, loss: 6.088485861255322e-06\n",
      "epoch: 260, loss: 5.827207132824697e-06\n",
      "epoch: 261, loss: 5.5702184909023345e-06\n",
      "epoch: 262, loss: 5.321022854332114e-06\n",
      "epoch: 263, loss: 5.082351890450809e-06\n",
      "epoch: 264, loss: 4.856663963437313e-06\n",
      "epoch: 265, loss: 4.645180524676107e-06\n",
      "epoch: 266, loss: 4.448721483640838e-06\n",
      "epoch: 267, loss: 4.267270924174227e-06\n",
      "epoch: 268, loss: 4.1002786019816995e-06\n",
      "epoch: 269, loss: 3.9468309296353254e-06\n",
      "epoch: 270, loss: 3.8056759876781143e-06\n",
      "epoch: 271, loss: 3.6754288430529414e-06\n",
      "epoch: 272, loss: 3.5547823244996835e-06\n",
      "epoch: 273, loss: 3.442478373472113e-06\n",
      "epoch: 274, loss: 3.337373300382751e-06\n",
      "epoch: 275, loss: 3.2385987651650794e-06\n",
      "epoch: 276, loss: 3.1453623705601785e-06\n",
      "epoch: 277, loss: 3.05714161186188e-06\n",
      "epoch: 278, loss: 2.973523805849254e-06\n",
      "epoch: 279, loss: 2.8941881282662507e-06\n",
      "epoch: 280, loss: 2.8189242584630847e-06\n",
      "epoch: 281, loss: 2.7474743546918035e-06\n",
      "epoch: 282, loss: 2.6796581096277805e-06\n",
      "epoch: 283, loss: 2.615164248709334e-06\n",
      "epoch: 284, loss: 2.553797685322934e-06\n",
      "epoch: 285, loss: 2.495216449460713e-06\n",
      "epoch: 286, loss: 2.4391231363551924e-06\n",
      "epoch: 287, loss: 2.385189645792707e-06\n",
      "epoch: 288, loss: 2.3331178908847505e-06\n",
      "epoch: 289, loss: 2.282645482409862e-06\n",
      "epoch: 290, loss: 2.233467739642947e-06\n",
      "epoch: 291, loss: 2.185381617891835e-06\n",
      "epoch: 292, loss: 2.13819930650061e-06\n",
      "epoch: 293, loss: 2.0918007521686377e-06\n",
      "epoch: 294, loss: 2.0461116037040483e-06\n",
      "epoch: 295, loss: 2.001044322241796e-06\n",
      "epoch: 296, loss: 1.956579808393144e-06\n",
      "epoch: 297, loss: 1.9127573978039436e-06\n",
      "epoch: 298, loss: 1.8695209291763604e-06\n",
      "epoch: 299, loss: 1.8269663542014314e-06\n",
      "epoch: 300, loss: 1.7850875337899197e-06\n",
      "epoch: 301, loss: 1.7438834447602858e-06\n",
      "epoch: 302, loss: 1.7034172969943029e-06\n",
      "epoch: 303, loss: 1.6636940927128308e-06\n",
      "epoch: 304, loss: 1.6247420262516243e-06\n",
      "epoch: 305, loss: 1.5865219893385074e-06\n",
      "epoch: 306, loss: 1.549078092466516e-06\n",
      "epoch: 307, loss: 1.512400103820255e-06\n",
      "epoch: 308, loss: 1.4764684692636365e-06\n",
      "epoch: 309, loss: 1.4412875088964938e-06\n",
      "epoch: 310, loss: 1.4068327800487168e-06\n",
      "epoch: 311, loss: 1.3731116723647574e-06\n",
      "epoch: 312, loss: 1.3401160003922996e-06\n",
      "epoch: 313, loss: 1.307821435148071e-06\n",
      "epoch: 314, loss: 1.2762310461766901e-06\n",
      "epoch: 315, loss: 1.2453460840333719e-06\n",
      "epoch: 316, loss: 1.2151276678196155e-06\n",
      "epoch: 317, loss: 1.1856086530315224e-06\n",
      "epoch: 318, loss: 1.156764824372658e-06\n",
      "epoch: 319, loss: 1.1285602568023023e-06\n",
      "epoch: 320, loss: 1.1010292837454472e-06\n",
      "epoch: 321, loss: 1.0741465530372807e-06\n",
      "epoch: 322, loss: 1.0478894409970962e-06\n",
      "epoch: 323, loss: 1.0222386208624812e-06\n",
      "epoch: 324, loss: 9.972170573746553e-07\n",
      "epoch: 325, loss: 9.727779115564772e-07\n",
      "epoch: 326, loss: 9.489106105320388e-07\n",
      "epoch: 327, loss: 9.255961117560219e-07\n",
      "epoch: 328, loss: 9.028429417412553e-07\n",
      "epoch: 329, loss: 8.806093205748766e-07\n",
      "epoch: 330, loss: 8.588900755057693e-07\n",
      "epoch: 331, loss: 8.376803179999115e-07\n",
      "epoch: 332, loss: 8.169496368282125e-07\n",
      "epoch: 333, loss: 7.966997941366571e-07\n",
      "epoch: 334, loss: 7.769173748783942e-07\n",
      "epoch: 335, loss: 7.57584246002807e-07\n",
      "epoch: 336, loss: 7.386907441286894e-07\n",
      "epoch: 337, loss: 7.202314691312495e-07\n",
      "epoch: 338, loss: 7.022148338364786e-07\n",
      "epoch: 339, loss: 6.845987741144199e-07\n",
      "epoch: 340, loss: 6.673948291791021e-07\n",
      "epoch: 341, loss: 6.505778173959698e-07\n",
      "epoch: 342, loss: 6.341670655274356e-07\n",
      "epoch: 343, loss: 6.181277854011569e-07\n",
      "epoch: 344, loss: 6.024691288075701e-07\n",
      "epoch: 345, loss: 5.871610255780979e-07\n",
      "epoch: 346, loss: 5.722272362618241e-07\n",
      "epoch: 347, loss: 5.57648490939755e-07\n",
      "epoch: 348, loss: 5.433931846710038e-07\n",
      "epoch: 349, loss: 5.294745051287464e-07\n",
      "epoch: 350, loss: 5.158848921382742e-07\n",
      "epoch: 351, loss: 5.026177518629993e-07\n",
      "epoch: 352, loss: 4.8965421228786e-07\n",
      "epoch: 353, loss: 4.77009905353043e-07\n",
      "epoch: 354, loss: 4.6465780201288e-07\n",
      "epoch: 355, loss: 4.5260725300977356e-07\n",
      "epoch: 356, loss: 4.4083316197429667e-07\n",
      "epoch: 357, loss: 4.293557935852732e-07\n",
      "epoch: 358, loss: 4.1814661244643503e-07\n",
      "epoch: 359, loss: 4.072049932801747e-07\n",
      "epoch: 360, loss: 3.965309076647827e-07\n",
      "epoch: 361, loss: 3.8612910202573403e-07\n",
      "epoch: 362, loss: 3.7596316815324826e-07\n",
      "epoch: 363, loss: 3.660564402707678e-07\n",
      "epoch: 364, loss: 3.5639226325656637e-07\n",
      "epoch: 365, loss: 3.4696074635576224e-07\n",
      "epoch: 366, loss: 3.3775819474612945e-07\n",
      "epoch: 367, loss: 3.287839831500605e-07\n",
      "epoch: 368, loss: 3.200368325906311e-07\n",
      "epoch: 369, loss: 3.1149392043516855e-07\n",
      "epoch: 370, loss: 3.031682354048826e-07\n",
      "epoch: 371, loss: 2.9505844167942996e-07\n",
      "epoch: 372, loss: 2.8713412802972016e-07\n",
      "epoch: 373, loss: 2.7941752023252775e-07\n",
      "epoch: 374, loss: 2.718919347444171e-07\n",
      "epoch: 375, loss: 2.64556831552909e-07\n",
      "epoch: 376, loss: 2.574045652181667e-07\n",
      "epoch: 377, loss: 2.5043419782377896e-07\n",
      "epoch: 378, loss: 2.4363913553315797e-07\n",
      "epoch: 379, loss: 2.3701389295638364e-07\n",
      "epoch: 380, loss: 2.3055523001858091e-07\n",
      "epoch: 381, loss: 2.2426782209095109e-07\n",
      "epoch: 382, loss: 2.1813073658449866e-07\n",
      "epoch: 383, loss: 2.1216033019300085e-07\n",
      "epoch: 384, loss: 2.063330128976304e-07\n",
      "epoch: 385, loss: 2.006580217539522e-07\n",
      "epoch: 386, loss: 1.951331540794854e-07\n",
      "epoch: 387, loss: 1.8974934334892168e-07\n",
      "epoch: 388, loss: 1.8450235472755594e-07\n",
      "epoch: 389, loss: 1.7939403562650114e-07\n",
      "epoch: 390, loss: 1.7441303157283983e-07\n",
      "epoch: 391, loss: 1.6956290949110553e-07\n",
      "epoch: 392, loss: 1.64835398663854e-07\n",
      "epoch: 393, loss: 1.602302859282645e-07\n",
      "epoch: 394, loss: 1.5575207612528175e-07\n",
      "epoch: 395, loss: 1.5139049480694666e-07\n",
      "epoch: 396, loss: 1.4714056817410892e-07\n",
      "epoch: 397, loss: 1.4300378836651362e-07\n",
      "epoch: 398, loss: 1.3897297890252958e-07\n",
      "epoch: 399, loss: 1.3505604101737845e-07\n",
      "epoch: 400, loss: 1.312344437565116e-07\n",
      "epoch: 401, loss: 1.2751745259720337e-07\n",
      "epoch: 402, loss: 1.2389878634166962e-07\n",
      "epoch: 403, loss: 1.2037519070418057e-07\n",
      "epoch: 404, loss: 1.1694537249695713e-07\n",
      "epoch: 405, loss: 1.1360870644239185e-07\n",
      "epoch: 406, loss: 1.1036040348244569e-07\n",
      "epoch: 407, loss: 1.0719969623096404e-07\n",
      "epoch: 408, loss: 1.0412465201170562e-07\n",
      "epoch: 409, loss: 1.0113262760569341e-07\n",
      "epoch: 410, loss: 9.822421986882546e-08\n",
      "epoch: 411, loss: 9.538733536373911e-08\n",
      "epoch: 412, loss: 9.262906530693726e-08\n",
      "epoch: 413, loss: 8.995107236842159e-08\n",
      "epoch: 414, loss: 8.733864831356186e-08\n",
      "epoch: 415, loss: 8.480532187604695e-08\n",
      "epoch: 416, loss: 8.233556059167313e-08\n",
      "epoch: 417, loss: 7.993565986907925e-08\n",
      "epoch: 418, loss: 7.759653897210228e-08\n",
      "epoch: 419, loss: 7.532710810664867e-08\n",
      "epoch: 420, loss: 7.312059580044661e-08\n",
      "epoch: 421, loss: 7.097159482327697e-08\n",
      "epoch: 422, loss: 6.887989911774639e-08\n",
      "epoch: 423, loss: 6.68518538304852e-08\n",
      "epoch: 424, loss: 6.487950088285288e-08\n",
      "epoch: 425, loss: 6.295958598911966e-08\n",
      "epoch: 426, loss: 6.109505079621158e-08\n",
      "epoch: 427, loss: 5.928392354803691e-08\n",
      "epoch: 428, loss: 5.75224916588013e-08\n",
      "epoch: 429, loss: 5.580510631375546e-08\n",
      "epoch: 430, loss: 5.4141249705708105e-08\n",
      "epoch: 431, loss: 5.2526104354910785e-08\n",
      "epoch: 432, loss: 5.095569122204324e-08\n",
      "epoch: 433, loss: 4.942630127402481e-08\n",
      "epoch: 434, loss: 4.7941867364897917e-08\n",
      "epoch: 435, loss: 4.6500943540195294e-08\n",
      "epoch: 436, loss: 4.509959694587451e-08\n",
      "epoch: 437, loss: 4.3737429678003537e-08\n",
      "epoch: 438, loss: 4.241418238848382e-08\n",
      "epoch: 439, loss: 4.11289953206051e-08\n",
      "epoch: 440, loss: 3.988279573263753e-08\n",
      "epoch: 441, loss: 3.8667518964530245e-08\n",
      "epoch: 442, loss: 3.7490284654495554e-08\n",
      "epoch: 443, loss: 3.6346897047678794e-08\n",
      "epoch: 444, loss: 3.523713942854556e-08\n",
      "epoch: 445, loss: 3.41595622899149e-08\n",
      "epoch: 446, loss: 3.3110438835137757e-08\n",
      "epoch: 447, loss: 3.209464338738144e-08\n",
      "epoch: 448, loss: 3.110705293352112e-08\n",
      "epoch: 449, loss: 3.014904947917785e-08\n",
      "epoch: 450, loss: 2.9218295338750977e-08\n",
      "epoch: 451, loss: 2.831584566820311e-08\n",
      "epoch: 452, loss: 2.7436970029270924e-08\n",
      "epoch: 453, loss: 2.6588475421363e-08\n",
      "epoch: 454, loss: 2.5761609734331614e-08\n",
      "epoch: 455, loss: 2.4959664557400174e-08\n",
      "epoch: 456, loss: 2.4180970115139644e-08\n",
      "epoch: 457, loss: 2.342560101453728e-08\n",
      "epoch: 458, loss: 2.2692015377856478e-08\n",
      "epoch: 459, loss: 2.198129145369876e-08\n",
      "epoch: 460, loss: 2.1291551632884875e-08\n",
      "epoch: 461, loss: 2.0621204299686724e-08\n",
      "epoch: 462, loss: 1.9971610143443286e-08\n",
      "epoch: 463, loss: 1.9341904078373773e-08\n",
      "epoch: 464, loss: 1.872990651463624e-08\n",
      "epoch: 465, loss: 1.8136573132210287e-08\n",
      "epoch: 466, loss: 1.7563321463853754e-08\n",
      "epoch: 467, loss: 1.70042255831504e-08\n",
      "epoch: 468, loss: 1.6463197027860588e-08\n",
      "epoch: 469, loss: 1.5940509356937582e-08\n",
      "epoch: 470, loss: 1.5431597333304126e-08\n",
      "epoch: 471, loss: 1.4936878400817477e-08\n",
      "epoch: 472, loss: 1.445939190602985e-08\n",
      "epoch: 473, loss: 1.3995594017046642e-08\n",
      "epoch: 474, loss: 1.3545212063093004e-08\n",
      "epoch: 475, loss: 1.3110559748952255e-08\n",
      "epoch: 476, loss: 1.26872414796253e-08\n",
      "epoch: 477, loss: 1.2277491912016103e-08\n",
      "epoch: 478, loss: 1.1880322503543539e-08\n",
      "epoch: 479, loss: 1.1495174589981616e-08\n",
      "epoch: 480, loss: 1.1123060694728792e-08\n",
      "epoch: 481, loss: 1.0763395508206486e-08\n",
      "epoch: 482, loss: 1.0412812834204033e-08\n",
      "epoch: 483, loss: 1.0072371381397716e-08\n",
      "epoch: 484, loss: 9.743811091311727e-09\n",
      "epoch: 485, loss: 9.42602618181354e-09\n",
      "epoch: 486, loss: 9.118212851433327e-09\n",
      "epoch: 487, loss: 8.819005969940008e-09\n",
      "epoch: 488, loss: 8.530335549039592e-09\n",
      "epoch: 489, loss: 8.250401251075345e-09\n",
      "epoch: 490, loss: 7.977973837114405e-09\n",
      "epoch: 491, loss: 7.715076577596847e-09\n",
      "epoch: 492, loss: 7.460569939610195e-09\n",
      "epoch: 493, loss: 7.213196262512156e-09\n",
      "epoch: 494, loss: 6.973281507782758e-09\n",
      "epoch: 495, loss: 6.74296396496743e-09\n",
      "epoch: 496, loss: 6.517165918040746e-09\n",
      "epoch: 497, loss: 6.300638233369682e-09\n",
      "epoch: 498, loss: 6.091632975824268e-09\n",
      "epoch: 499, loss: 5.8871174601904386e-09\n",
      "epoch: 500, loss: 5.690403259706045e-09\n",
      "epoch: 501, loss: 5.5001665444365244e-09\n",
      "epoch: 502, loss: 5.314564788250209e-09\n",
      "epoch: 503, loss: 5.136202574362869e-09\n",
      "epoch: 504, loss: 4.962704025501807e-09\n",
      "epoch: 505, loss: 4.795016828040843e-09\n",
      "epoch: 506, loss: 4.633099681683461e-09\n",
      "epoch: 507, loss: 4.476313542056687e-09\n",
      "epoch: 508, loss: 4.324920865883541e-09\n",
      "epoch: 509, loss: 4.1780774395761e-09\n",
      "epoch: 510, loss: 4.0360710329423455e-09\n",
      "epoch: 511, loss: 3.898740441599102e-09\n",
      "epoch: 512, loss: 3.766740697130899e-09\n",
      "epoch: 513, loss: 3.638256584892474e-09\n",
      "epoch: 514, loss: 3.513507706998098e-09\n",
      "epoch: 515, loss: 3.3933413856601646e-09\n",
      "epoch: 516, loss: 3.2768734392618626e-09\n",
      "epoch: 517, loss: 3.16422821278195e-09\n",
      "epoch: 518, loss: 3.0558562347238194e-09\n",
      "epoch: 519, loss: 2.950066635420967e-09\n",
      "epoch: 520, loss: 2.8482696201592717e-09\n",
      "epoch: 521, loss: 2.7500965948945577e-09\n",
      "epoch: 522, loss: 2.6556654653120404e-09\n",
      "epoch: 523, loss: 2.562904555247769e-09\n",
      "epoch: 524, loss: 2.4736463988261903e-09\n",
      "epoch: 525, loss: 2.3882524846641218e-09\n",
      "epoch: 526, loss: 2.305080126774328e-09\n",
      "epoch: 527, loss: 2.224657569271926e-09\n",
      "epoch: 528, loss: 2.1471093791802787e-09\n",
      "epoch: 529, loss: 2.071803839598374e-09\n",
      "epoch: 530, loss: 1.9990029631600237e-09\n",
      "epoch: 531, loss: 1.9294243980283454e-09\n",
      "epoch: 532, loss: 1.8612695829034465e-09\n",
      "epoch: 533, loss: 1.796391924102636e-09\n",
      "epoch: 534, loss: 1.7328203316679947e-09\n",
      "epoch: 535, loss: 1.6716290573981496e-09\n",
      "epoch: 536, loss: 1.6130099478317561e-09\n",
      "epoch: 537, loss: 1.5554804111417297e-09\n",
      "epoch: 538, loss: 1.500362944995004e-09\n",
      "epoch: 539, loss: 1.4470282749812213e-09\n",
      "epoch: 540, loss: 1.3956908961887393e-09\n",
      "epoch: 541, loss: 1.3456475933537604e-09\n",
      "epoch: 542, loss: 1.2971350660251346e-09\n",
      "epoch: 543, loss: 1.2507476165879439e-09\n",
      "epoch: 544, loss: 1.206255983987603e-09\n",
      "epoch: 545, loss: 1.1630234553194896e-09\n",
      "epoch: 546, loss: 1.1214671413739552e-09\n",
      "epoch: 547, loss: 1.0815403017616632e-09\n",
      "epoch: 548, loss: 1.0424823226884428e-09\n",
      "epoch: 549, loss: 1.0048950560559433e-09\n",
      "epoch: 550, loss: 9.682852297743239e-10\n",
      "epoch: 551, loss: 9.333043227144344e-10\n",
      "epoch: 552, loss: 8.993446543037464e-10\n",
      "epoch: 553, loss: 8.669337470124105e-10\n",
      "epoch: 554, loss: 8.353677749539656e-10\n",
      "epoch: 555, loss: 8.048733346477377e-10\n",
      "epoch: 556, loss: 7.754055175723806e-10\n",
      "epoch: 557, loss: 7.472034657673987e-10\n",
      "epoch: 558, loss: 7.198838192223889e-10\n",
      "epoch: 559, loss: 6.937050933686351e-10\n",
      "epoch: 560, loss: 6.68359256827955e-10\n",
      "epoch: 561, loss: 6.440467048562937e-10\n",
      "epoch: 562, loss: 6.202587332637677e-10\n",
      "epoch: 563, loss: 5.974216676918331e-10\n",
      "epoch: 564, loss: 5.753138521136236e-10\n",
      "epoch: 565, loss: 5.54001733377163e-10\n",
      "epoch: 566, loss: 5.336711628167734e-10\n",
      "epoch: 567, loss: 5.139103032014702e-10\n",
      "epoch: 568, loss: 4.949477494520238e-10\n",
      "epoch: 569, loss: 4.766416705770382e-10\n",
      "epoch: 570, loss: 4.5891740407810744e-10\n",
      "epoch: 571, loss: 4.4185555214681926e-10\n",
      "epoch: 572, loss: 4.25213753096898e-10\n",
      "epoch: 573, loss: 4.0940961731905645e-10\n",
      "epoch: 574, loss: 3.9399822293617603e-10\n",
      "epoch: 575, loss: 3.7944247743837423e-10\n",
      "epoch: 576, loss: 3.651364210988106e-10\n",
      "epoch: 577, loss: 3.514648572178203e-10\n",
      "epoch: 578, loss: 3.382503166449169e-10\n",
      "epoch: 579, loss: 3.2550412365495163e-10\n",
      "epoch: 580, loss: 3.1326241600737603e-10\n",
      "epoch: 581, loss: 3.0130681283324634e-10\n",
      "epoch: 582, loss: 2.900005791062199e-10\n",
      "epoch: 583, loss: 2.7911895017496136e-10\n",
      "epoch: 584, loss: 2.685363598153856e-10\n",
      "epoch: 585, loss: 2.581137525936583e-10\n",
      "epoch: 586, loss: 2.482428151928673e-10\n",
      "epoch: 587, loss: 2.387264275149903e-10\n",
      "epoch: 588, loss: 2.296813850222179e-10\n",
      "epoch: 589, loss: 2.209312732759372e-10\n",
      "epoch: 590, loss: 2.1241214342992976e-10\n",
      "epoch: 591, loss: 2.0432616709697982e-10\n",
      "epoch: 592, loss: 1.964206575166827e-10\n",
      "epoch: 593, loss: 1.8884382946282585e-10\n",
      "epoch: 594, loss: 1.8167894966225617e-10\n",
      "epoch: 595, loss: 1.7458268164460833e-10\n",
      "epoch: 596, loss: 1.6765128174611732e-10\n",
      "epoch: 597, loss: 1.612313060839199e-10\n",
      "epoch: 598, loss: 1.5500917216471066e-10\n",
      "epoch: 599, loss: 1.4903744904870564e-10\n",
      "epoch: 600, loss: 1.43151990261714e-10\n",
      "epoch: 601, loss: 1.3745921068064604e-10\n",
      "epoch: 602, loss: 1.322438270001669e-10\n",
      "epoch: 603, loss: 1.269571114903556e-10\n",
      "epoch: 604, loss: 1.2211265332240373e-10\n",
      "epoch: 605, loss: 1.1723094717197569e-10\n",
      "epoch: 606, loss: 1.1263462385002754e-10\n",
      "epoch: 607, loss: 1.0826761709381572e-10\n",
      "epoch: 608, loss: 1.0403511385703723e-10\n",
      "epoch: 609, loss: 9.986550475460376e-11\n",
      "epoch: 610, loss: 9.603945416714055e-11\n",
      "epoch: 611, loss: 9.225337160856384e-11\n",
      "epoch: 612, loss: 8.860162603596677e-11\n",
      "epoch: 613, loss: 8.497275105767699e-11\n",
      "epoch: 614, loss: 8.166628484573835e-11\n",
      "epoch: 615, loss: 7.83010323246458e-11\n",
      "epoch: 616, loss: 7.531419932149674e-11\n",
      "epoch: 617, loss: 7.223360798391809e-11\n",
      "epoch: 618, loss: 6.950640063507763e-11\n",
      "epoch: 619, loss: 6.675576758041757e-11\n",
      "epoch: 620, loss: 6.405631580719273e-11\n",
      "epoch: 621, loss: 6.143646702483352e-11\n",
      "epoch: 622, loss: 5.892580867694619e-11\n",
      "epoch: 623, loss: 5.65487656700725e-11\n",
      "epoch: 624, loss: 5.433259397946699e-11\n",
      "epoch: 625, loss: 5.2218340762522075e-11\n",
      "epoch: 626, loss: 5.011824288914113e-11\n",
      "epoch: 627, loss: 4.8037684940993586e-11\n",
      "epoch: 628, loss: 4.62133109557783e-11\n",
      "epoch: 629, loss: 4.432904043838448e-11\n",
      "epoch: 630, loss: 4.250316765208595e-11\n",
      "epoch: 631, loss: 4.078348769809281e-11\n",
      "epoch: 632, loss: 3.912392632088313e-11\n",
      "epoch: 633, loss: 3.751854382727515e-11\n",
      "epoch: 634, loss: 3.5967340217268884e-11\n",
      "epoch: 635, loss: 3.4569680451568274e-11\n",
      "epoch: 636, loss: 3.3194114124057705e-11\n",
      "epoch: 637, loss: 3.178129981407096e-11\n",
      "epoch: 638, loss: 3.04430369801878e-11\n",
      "epoch: 639, loss: 2.9287516856157936e-11\n",
      "epoch: 640, loss: 2.8053392941984612e-11\n",
      "epoch: 641, loss: 2.6945834452618556e-11\n",
      "epoch: 642, loss: 2.5808077896982695e-11\n",
      "epoch: 643, loss: 2.479894067874966e-11\n",
      "epoch: 644, loss: 2.373445884273906e-11\n",
      "epoch: 645, loss: 2.274985755335024e-11\n",
      "epoch: 646, loss: 2.1829482665935984e-11\n",
      "epoch: 647, loss: 2.0988766280538584e-11\n",
      "epoch: 648, loss: 2.0165702441232725e-11\n",
      "epoch: 649, loss: 1.9282186958236025e-11\n",
      "epoch: 650, loss: 1.849687070176742e-11\n",
      "epoch: 651, loss: 1.772454405468693e-11\n",
      "epoch: 652, loss: 1.6990242546199852e-11\n",
      "epoch: 653, loss: 1.6313284056934663e-11\n",
      "epoch: 654, loss: 1.5621504090290728e-11\n",
      "epoch: 655, loss: 1.4973411399665792e-11\n",
      "epoch: 656, loss: 1.4339196496848672e-11\n",
      "epoch: 657, loss: 1.3718859381839366e-11\n",
      "epoch: 658, loss: 1.3181677971374484e-11\n",
      "epoch: 659, loss: 1.2648659897251946e-11\n",
      "epoch: 660, loss: 1.2169931729033578e-11\n",
      "epoch: 661, loss: 1.162586693581602e-11\n",
      "epoch: 662, loss: 1.1186163106913227e-11\n",
      "epoch: 663, loss: 1.0657474902586728e-11\n",
      "epoch: 664, loss: 1.0224987523343998e-11\n",
      "epoch: 665, loss: 9.820866342380441e-12\n",
      "epoch: 666, loss: 9.403700040877538e-12\n",
      "epoch: 667, loss: 8.960887587505795e-12\n",
      "epoch: 668, loss: 8.63809024309603e-12\n",
      "epoch: 669, loss: 8.25767232370822e-12\n",
      "epoch: 670, loss: 7.89485143926072e-12\n",
      "epoch: 671, loss: 7.578937477603631e-12\n",
      "epoch: 672, loss: 7.252254352607679e-12\n",
      "epoch: 673, loss: 6.961209386702194e-12\n",
      "epoch: 674, loss: 6.671219132670103e-12\n",
      "epoch: 675, loss: 6.357192550154878e-12\n",
      "epoch: 676, loss: 6.082745418467539e-12\n",
      "epoch: 677, loss: 5.8176241601870515e-12\n",
      "epoch: 678, loss: 5.590305995895051e-12\n",
      "epoch: 679, loss: 5.34666755314106e-12\n",
      "epoch: 680, loss: 5.1359472230672054e-12\n",
      "epoch: 681, loss: 4.926836716379057e-12\n",
      "epoch: 682, loss: 4.725220215107129e-12\n",
      "epoch: 683, loss: 4.508005080339217e-12\n",
      "epoch: 684, loss: 4.29278834701563e-12\n",
      "epoch: 685, loss: 4.089451000055533e-12\n",
      "epoch: 686, loss: 3.90176779774265e-12\n",
      "epoch: 687, loss: 3.741895682196628e-12\n",
      "epoch: 688, loss: 3.5603187065191833e-12\n",
      "epoch: 689, loss: 3.4364178169710158e-12\n",
      "epoch: 690, loss: 3.311351193246992e-12\n",
      "epoch: 691, loss: 3.1814550993658486e-12\n",
      "epoch: 692, loss: 3.0509483828211614e-12\n",
      "epoch: 693, loss: 2.904010365512022e-12\n",
      "epoch: 694, loss: 2.800482068465726e-12\n",
      "epoch: 695, loss: 2.6677549058717887e-12\n",
      "epoch: 696, loss: 2.5607294062979236e-12\n",
      "epoch: 697, loss: 2.4638069362481474e-12\n",
      "epoch: 698, loss: 2.334132886971929e-12\n",
      "epoch: 699, loss: 2.226274720129595e-12\n",
      "epoch: 700, loss: 2.1363466551349575e-12\n",
      "epoch: 701, loss: 2.026434575697067e-12\n",
      "epoch: 702, loss: 1.9615420399077266e-12\n",
      "epoch: 703, loss: 1.873390331752489e-12\n",
      "epoch: 704, loss: 1.7804646645913635e-12\n",
      "epoch: 705, loss: 1.7026935417163713e-12\n",
      "epoch: 706, loss: 1.650291014954064e-12\n",
      "epoch: 707, loss: 1.5687451337953462e-12\n",
      "epoch: 708, loss: 1.502409308073993e-12\n",
      "epoch: 709, loss: 1.4375167722846527e-12\n",
      "epoch: 710, loss: 1.3891110484109959e-12\n",
      "epoch: 711, loss: 1.3232193118994928e-12\n",
      "epoch: 712, loss: 1.2645995361992846e-12\n",
      "epoch: 713, loss: 1.2230771950783037e-12\n",
      "epoch: 714, loss: 1.1542988787027753e-12\n",
      "epoch: 715, loss: 1.1076695116685187e-12\n",
      "epoch: 716, loss: 1.0521028492860296e-12\n",
      "epoch: 717, loss: 1.014743844507393e-12\n",
      "epoch: 718, loss: 9.68669588985449e-13\n",
      "epoch: 719, loss: 9.291456493087935e-13\n",
      "epoch: 720, loss: 8.850697952311748e-13\n",
      "epoch: 721, loss: 8.611444890505027e-13\n",
      "epoch: 722, loss: 8.154033004359462e-13\n",
      "epoch: 723, loss: 7.756573161543656e-13\n",
      "epoch: 724, loss: 7.35467242662935e-13\n",
      "epoch: 725, loss: 6.963873921961294e-13\n",
      "epoch: 726, loss: 6.628586568524497e-13\n",
      "epoch: 727, loss: 6.257772078299695e-13\n",
      "epoch: 728, loss: 6.055711487817916e-13\n",
      "epoch: 729, loss: 5.773714839563127e-13\n",
      "epoch: 730, loss: 5.5827564793276e-13\n",
      "epoch: 731, loss: 5.304645611658998e-13\n",
      "epoch: 732, loss: 5.067612995901527e-13\n",
      "epoch: 733, loss: 4.806155473602303e-13\n",
      "epoch: 734, loss: 4.637401573859279e-13\n",
      "epoch: 735, loss: 4.3365311341858614e-13\n",
      "epoch: 736, loss: 4.4153569689342476e-13\n",
      "epoch: 737, loss: 4.2060799287924056e-13\n",
      "epoch: 738, loss: 3.8125058665627876e-13\n",
      "epoch: 739, loss: 3.6021186033963204e-13\n",
      "epoch: 740, loss: 3.432809592140984e-13\n",
      "epoch: 741, loss: 3.2690516960087734e-13\n",
      "epoch: 742, loss: 3.153588501447757e-13\n",
      "epoch: 743, loss: 3.0137004003449874e-13\n",
      "epoch: 744, loss: 2.902678097882472e-13\n",
      "epoch: 745, loss: 2.7622348852673895e-13\n",
      "epoch: 746, loss: 2.6872948311051914e-13\n",
      "epoch: 747, loss: 2.558508960248673e-13\n",
      "epoch: 748, loss: 2.456368441983159e-13\n",
      "epoch: 749, loss: 2.2987167724863866e-13\n",
      "epoch: 750, loss: 2.216560268664125e-13\n",
      "epoch: 751, loss: 2.0855539517583566e-13\n",
      "epoch: 752, loss: 1.9922952176898434e-13\n",
      "epoch: 753, loss: 1.9723112032465906e-13\n",
      "epoch: 754, loss: 1.8768320231288271e-13\n",
      "epoch: 755, loss: 1.812439087700568e-13\n",
      "epoch: 756, loss: 1.7524870443708096e-13\n",
      "epoch: 757, loss: 1.670330540548548e-13\n",
      "epoch: 758, loss: 1.5987211554602254e-13\n",
      "epoch: 759, loss: 1.4943601911454607e-13\n",
      "epoch: 760, loss: 1.417754802446325e-13\n",
      "epoch: 761, loss: 1.3200551762793111e-13\n",
      "epoch: 762, loss: 1.272870697732742e-13\n",
      "epoch: 763, loss: 1.156852391659413e-13\n",
      "epoch: 764, loss: 1.0891287871572786e-13\n",
      "epoch: 765, loss: 1.0946799022804043e-13\n",
      "epoch: 766, loss: 1.053046538856961e-13\n",
      "epoch: 767, loss: 9.753309271332e-14\n",
      "epoch: 768, loss: 9.375833442959447e-14\n",
      "epoch: 769, loss: 9.0427665355719e-14\n",
      "epoch: 770, loss: 8.754108549169359e-14\n",
      "epoch: 771, loss: 8.509859483751825e-14\n",
      "epoch: 772, loss: 8.759659664292485e-14\n",
      "epoch: 773, loss: 7.710498906021712e-14\n",
      "epoch: 774, loss: 7.327471962526033e-14\n",
      "epoch: 775, loss: 6.711298183859071e-14\n",
      "epoch: 776, loss: 6.356026815979021e-14\n",
      "epoch: 777, loss: 5.934142066621462e-14\n",
      "epoch: 778, loss: 5.601075159233915e-14\n",
      "epoch: 779, loss: 5.601075159233915e-14\n",
      "epoch: 780, loss: 5.601075159233915e-14\n",
      "epoch: 781, loss: 5.601075159233915e-14\n",
      "epoch: 782, loss: 5.601075159233915e-14\n",
      "epoch: 783, loss: 5.601075159233915e-14\n",
      "epoch: 784, loss: 5.601075159233915e-14\n",
      "epoch: 785, loss: 5.601075159233915e-14\n",
      "epoch: 786, loss: 5.689893001203927e-14\n",
      "epoch: 787, loss: 5.689893001203927e-14\n",
      "epoch: 788, loss: 5.689893001203927e-14\n",
      "epoch: 789, loss: 5.689893001203927e-14\n",
      "epoch: 790, loss: 5.689893001203927e-14\n",
      "epoch: 791, loss: 5.689893001203927e-14\n",
      "epoch: 792, loss: 5.689893001203927e-14\n",
      "epoch: 793, loss: 5.689893001203927e-14\n",
      "epoch: 794, loss: 5.689893001203927e-14\n",
      "epoch: 795, loss: 5.601075159233915e-14\n",
      "epoch: 796, loss: 5.601075159233915e-14\n",
      "epoch: 797, loss: 5.601075159233915e-14\n",
      "epoch: 798, loss: 5.601075159233915e-14\n",
      "epoch: 799, loss: 5.601075159233915e-14\n",
      "epoch: 800, loss: 5.601075159233915e-14\n",
      "epoch: 801, loss: 5.601075159233915e-14\n",
      "epoch: 802, loss: 5.601075159233915e-14\n",
      "epoch: 803, loss: 5.601075159233915e-14\n",
      "epoch: 804, loss: 5.601075159233915e-14\n",
      "epoch: 805, loss: 5.601075159233915e-14\n",
      "epoch: 806, loss: 5.601075159233915e-14\n",
      "epoch: 807, loss: 5.601075159233915e-14\n",
      "epoch: 808, loss: 5.601075159233915e-14\n",
      "epoch: 809, loss: 5.601075159233915e-14\n",
      "epoch: 810, loss: 5.601075159233915e-14\n",
      "epoch: 811, loss: 5.601075159233915e-14\n",
      "epoch: 812, loss: 5.601075159233915e-14\n",
      "epoch: 813, loss: 5.601075159233915e-14\n",
      "epoch: 814, loss: 5.601075159233915e-14\n",
      "epoch: 815, loss: 5.601075159233915e-14\n",
      "epoch: 816, loss: 5.601075159233915e-14\n",
      "epoch: 817, loss: 5.601075159233915e-14\n",
      "epoch: 818, loss: 5.601075159233915e-14\n",
      "epoch: 819, loss: 5.601075159233915e-14\n",
      "epoch: 820, loss: 5.601075159233915e-14\n",
      "epoch: 821, loss: 5.601075159233915e-14\n",
      "epoch: 822, loss: 5.601075159233915e-14\n",
      "epoch: 823, loss: 5.601075159233915e-14\n",
      "epoch: 824, loss: 5.601075159233915e-14\n",
      "epoch: 825, loss: 5.601075159233915e-14\n",
      "epoch: 826, loss: 5.601075159233915e-14\n",
      "epoch: 827, loss: 5.601075159233915e-14\n",
      "epoch: 828, loss: 5.601075159233915e-14\n",
      "epoch: 829, loss: 5.601075159233915e-14\n",
      "epoch: 830, loss: 5.601075159233915e-14\n",
      "epoch: 831, loss: 5.601075159233915e-14\n",
      "epoch: 832, loss: 5.601075159233915e-14\n",
      "epoch: 833, loss: 5.601075159233915e-14\n",
      "epoch: 834, loss: 5.601075159233915e-14\n",
      "epoch: 835, loss: 5.601075159233915e-14\n",
      "epoch: 836, loss: 5.601075159233915e-14\n",
      "epoch: 837, loss: 5.601075159233915e-14\n",
      "epoch: 838, loss: 5.601075159233915e-14\n",
      "epoch: 839, loss: 5.601075159233915e-14\n",
      "epoch: 840, loss: 5.601075159233915e-14\n",
      "epoch: 841, loss: 5.601075159233915e-14\n",
      "epoch: 842, loss: 5.601075159233915e-14\n",
      "epoch: 843, loss: 5.601075159233915e-14\n",
      "epoch: 844, loss: 5.601075159233915e-14\n",
      "epoch: 845, loss: 5.601075159233915e-14\n",
      "epoch: 846, loss: 5.601075159233915e-14\n",
      "epoch: 847, loss: 5.601075159233915e-14\n",
      "epoch: 848, loss: 5.601075159233915e-14\n",
      "epoch: 849, loss: 5.601075159233915e-14\n",
      "epoch: 850, loss: 5.601075159233915e-14\n",
      "epoch: 851, loss: 5.601075159233915e-14\n",
      "epoch: 852, loss: 5.601075159233915e-14\n",
      "epoch: 853, loss: 5.601075159233915e-14\n",
      "epoch: 854, loss: 5.601075159233915e-14\n",
      "epoch: 855, loss: 5.601075159233915e-14\n",
      "epoch: 856, loss: 5.601075159233915e-14\n",
      "epoch: 857, loss: 5.601075159233915e-14\n",
      "epoch: 858, loss: 5.601075159233915e-14\n",
      "epoch: 859, loss: 5.601075159233915e-14\n",
      "epoch: 860, loss: 5.601075159233915e-14\n",
      "epoch: 861, loss: 5.601075159233915e-14\n",
      "epoch: 862, loss: 5.601075159233915e-14\n",
      "epoch: 863, loss: 5.601075159233915e-14\n",
      "epoch: 864, loss: 5.601075159233915e-14\n",
      "epoch: 865, loss: 5.601075159233915e-14\n",
      "epoch: 866, loss: 5.601075159233915e-14\n",
      "epoch: 867, loss: 5.601075159233915e-14\n",
      "epoch: 868, loss: 5.601075159233915e-14\n",
      "epoch: 869, loss: 5.601075159233915e-14\n",
      "epoch: 870, loss: 5.601075159233915e-14\n",
      "epoch: 871, loss: 5.601075159233915e-14\n",
      "epoch: 872, loss: 5.601075159233915e-14\n",
      "epoch: 873, loss: 5.601075159233915e-14\n",
      "epoch: 874, loss: 5.601075159233915e-14\n",
      "epoch: 875, loss: 5.601075159233915e-14\n",
      "epoch: 876, loss: 5.601075159233915e-14\n",
      "epoch: 877, loss: 5.601075159233915e-14\n",
      "epoch: 878, loss: 5.601075159233915e-14\n",
      "epoch: 879, loss: 5.601075159233915e-14\n",
      "epoch: 880, loss: 5.601075159233915e-14\n",
      "epoch: 881, loss: 5.601075159233915e-14\n",
      "epoch: 882, loss: 5.601075159233915e-14\n",
      "epoch: 883, loss: 5.601075159233915e-14\n",
      "epoch: 884, loss: 5.601075159233915e-14\n",
      "epoch: 885, loss: 5.601075159233915e-14\n",
      "epoch: 886, loss: 5.601075159233915e-14\n",
      "epoch: 887, loss: 5.601075159233915e-14\n",
      "epoch: 888, loss: 5.601075159233915e-14\n",
      "epoch: 889, loss: 5.601075159233915e-14\n",
      "epoch: 890, loss: 5.601075159233915e-14\n",
      "epoch: 891, loss: 5.601075159233915e-14\n",
      "epoch: 892, loss: 5.601075159233915e-14\n",
      "epoch: 893, loss: 5.601075159233915e-14\n",
      "epoch: 894, loss: 5.601075159233915e-14\n",
      "epoch: 895, loss: 5.601075159233915e-14\n",
      "epoch: 896, loss: 5.601075159233915e-14\n",
      "epoch: 897, loss: 5.601075159233915e-14\n",
      "epoch: 898, loss: 5.601075159233915e-14\n",
      "epoch: 899, loss: 5.601075159233915e-14\n",
      "epoch: 900, loss: 5.601075159233915e-14\n",
      "epoch: 901, loss: 5.601075159233915e-14\n",
      "epoch: 902, loss: 5.601075159233915e-14\n",
      "epoch: 903, loss: 5.601075159233915e-14\n",
      "epoch: 904, loss: 5.601075159233915e-14\n",
      "epoch: 905, loss: 5.601075159233915e-14\n",
      "epoch: 906, loss: 5.601075159233915e-14\n",
      "epoch: 907, loss: 5.601075159233915e-14\n",
      "epoch: 908, loss: 5.601075159233915e-14\n",
      "epoch: 909, loss: 5.601075159233915e-14\n",
      "epoch: 910, loss: 5.601075159233915e-14\n",
      "epoch: 911, loss: 5.601075159233915e-14\n",
      "epoch: 912, loss: 5.601075159233915e-14\n",
      "epoch: 913, loss: 5.601075159233915e-14\n",
      "epoch: 914, loss: 5.601075159233915e-14\n",
      "epoch: 915, loss: 5.601075159233915e-14\n",
      "epoch: 916, loss: 5.601075159233915e-14\n",
      "epoch: 917, loss: 5.601075159233915e-14\n",
      "epoch: 918, loss: 5.601075159233915e-14\n",
      "epoch: 919, loss: 5.601075159233915e-14\n",
      "epoch: 920, loss: 5.601075159233915e-14\n",
      "epoch: 921, loss: 5.601075159233915e-14\n",
      "epoch: 922, loss: 5.601075159233915e-14\n",
      "epoch: 923, loss: 5.601075159233915e-14\n",
      "epoch: 924, loss: 5.601075159233915e-14\n",
      "epoch: 925, loss: 5.601075159233915e-14\n",
      "epoch: 926, loss: 5.601075159233915e-14\n",
      "epoch: 927, loss: 5.601075159233915e-14\n",
      "epoch: 928, loss: 5.601075159233915e-14\n",
      "epoch: 929, loss: 5.601075159233915e-14\n",
      "epoch: 930, loss: 5.601075159233915e-14\n",
      "epoch: 931, loss: 5.601075159233915e-14\n",
      "epoch: 932, loss: 5.601075159233915e-14\n",
      "epoch: 933, loss: 5.601075159233915e-14\n",
      "epoch: 934, loss: 5.601075159233915e-14\n",
      "epoch: 935, loss: 5.601075159233915e-14\n",
      "epoch: 936, loss: 5.601075159233915e-14\n",
      "epoch: 937, loss: 5.601075159233915e-14\n",
      "epoch: 938, loss: 5.601075159233915e-14\n",
      "epoch: 939, loss: 5.601075159233915e-14\n",
      "epoch: 940, loss: 5.601075159233915e-14\n",
      "epoch: 941, loss: 5.601075159233915e-14\n",
      "epoch: 942, loss: 5.601075159233915e-14\n",
      "epoch: 943, loss: 5.601075159233915e-14\n",
      "epoch: 944, loss: 5.601075159233915e-14\n",
      "epoch: 945, loss: 5.601075159233915e-14\n",
      "epoch: 946, loss: 5.601075159233915e-14\n",
      "epoch: 947, loss: 5.601075159233915e-14\n",
      "epoch: 948, loss: 5.601075159233915e-14\n",
      "epoch: 949, loss: 5.601075159233915e-14\n",
      "epoch: 950, loss: 5.601075159233915e-14\n",
      "epoch: 951, loss: 5.601075159233915e-14\n",
      "epoch: 952, loss: 5.601075159233915e-14\n",
      "epoch: 953, loss: 5.601075159233915e-14\n",
      "epoch: 954, loss: 5.601075159233915e-14\n",
      "epoch: 955, loss: 5.601075159233915e-14\n",
      "epoch: 956, loss: 5.601075159233915e-14\n",
      "epoch: 957, loss: 5.601075159233915e-14\n",
      "epoch: 958, loss: 5.601075159233915e-14\n",
      "epoch: 959, loss: 5.601075159233915e-14\n",
      "epoch: 960, loss: 5.601075159233915e-14\n",
      "epoch: 961, loss: 5.601075159233915e-14\n",
      "epoch: 962, loss: 5.601075159233915e-14\n",
      "epoch: 963, loss: 5.601075159233915e-14\n",
      "epoch: 964, loss: 5.601075159233915e-14\n",
      "epoch: 965, loss: 5.601075159233915e-14\n",
      "epoch: 966, loss: 5.601075159233915e-14\n",
      "epoch: 967, loss: 5.601075159233915e-14\n",
      "epoch: 968, loss: 5.601075159233915e-14\n",
      "epoch: 969, loss: 5.601075159233915e-14\n",
      "epoch: 970, loss: 5.601075159233915e-14\n",
      "epoch: 971, loss: 5.601075159233915e-14\n",
      "epoch: 972, loss: 5.601075159233915e-14\n",
      "epoch: 973, loss: 5.601075159233915e-14\n",
      "epoch: 974, loss: 5.601075159233915e-14\n",
      "epoch: 975, loss: 5.601075159233915e-14\n",
      "epoch: 976, loss: 5.601075159233915e-14\n",
      "epoch: 977, loss: 5.601075159233915e-14\n",
      "epoch: 978, loss: 5.601075159233915e-14\n",
      "epoch: 979, loss: 5.601075159233915e-14\n",
      "epoch: 980, loss: 5.601075159233915e-14\n",
      "epoch: 981, loss: 5.601075159233915e-14\n",
      "epoch: 982, loss: 5.601075159233915e-14\n",
      "epoch: 983, loss: 5.440092820663267e-14\n",
      "epoch: 984, loss: 5.440092820663267e-14\n",
      "epoch: 985, loss: 5.440092820663267e-14\n",
      "epoch: 986, loss: 5.440092820663267e-14\n",
      "epoch: 987, loss: 5.440092820663267e-14\n",
      "epoch: 988, loss: 5.440092820663267e-14\n",
      "epoch: 989, loss: 5.440092820663267e-14\n",
      "epoch: 990, loss: 5.440092820663267e-14\n",
      "epoch: 991, loss: 5.440092820663267e-14\n",
      "epoch: 992, loss: 5.440092820663267e-14\n",
      "epoch: 993, loss: 5.440092820663267e-14\n",
      "epoch: 994, loss: 5.440092820663267e-14\n",
      "epoch: 995, loss: 5.440092820663267e-14\n",
      "epoch: 996, loss: 5.440092820663267e-14\n",
      "epoch: 997, loss: 5.295763827461997e-14\n",
      "epoch: 998, loss: 5.295763827461997e-14\n",
      "epoch: 999, loss: 5.295763827461997e-14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(EPOCH):\n",
    "    y_pred = model(x)       # get prediction\n",
    "    loss = loss_fun(y_pred, y)  # get the loss\n",
    "\n",
    "    print(f\"epoch: {epoch}, loss: {loss.item()}\")       # output the loss\n",
    "    # .item(): Returns the value of this tensor as a standard Python number.\n",
    "\n",
    "    # reset all GD, Resets the gradients of all optimized torch.Tensor s.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()             # Computes the gradient of current tensor\n",
    "    # ajust the paramter, Perform a single optimization step.\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2352e-07],\n",
      "        [-4.0233e-07],\n",
      "        [ 1.0000e+00],\n",
      "        [ 1.0000e+00]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "y_pred = model(x)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4797, -0.0278],\n",
      "        [-0.6809,  0.6809],\n",
      "        [-1.1663,  1.1766],\n",
      "        [ 0.2497, -0.1020],\n",
      "        [ 1.4542, -1.4576]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5920,  0.5812, -0.0223, -0.3972, -0.0073], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4159,  0.2393,  0.7252, -0.0265,  0.7873]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1391], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.parameters():\n",
    "    print(f\"{layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- output explain\n",
    "\n",
    "```sh\n",
    "Parameter containing:   # w1, the parameters of weight for the input to hidden, 5*2, 5 hidden * 2 feature\n",
    "tensor([[ 0.9250, -0.9250],\n",
    "       [-0.6403, -0.3083],\n",
    "       [-1.1184,  1.1062],\n",
    "       [ 0.8657, -0.8809],\n",
    "       [-0.4945,  0.2859]], requires_grad=True)\n",
    "Parameter containing:       # w2, the parameters of weight for the hidden to output, 1*5, 1 output * 5 hidden\n",
    "tensor([ 0.0132, -0.5322, -0.0052, -0.0107, -0.4866], requires_grad=True)\n",
    "Parameter containing:       # b1, the parameters of bias for the input to hidden, 1*5, 1 bias * 5 hidden\n",
    "tensor([[0.6443, 0.1278, 0.9159, 0.4725, 0.2167]], requires_grad=True)\n",
    "Parameter containing:       # b2, the parameters of bias for the hidden to output, 1*1, 1 output * 1 bias\n",
    "tensor([-0.0085], requires_grad=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To less epoch:\n",
    "  - more hidden layer\n",
    "  - lr\n",
    "  - activation func / loss func\n",
    "  - algorithm / optimizer\n",
    "  - more multilayer in the model:\n",
    "    - more laze+relu\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
